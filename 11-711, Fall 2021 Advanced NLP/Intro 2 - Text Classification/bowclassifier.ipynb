{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Bag-of-Words (BoW) Sentiment Classifier\n",
    "\n",
    "This notebook is using the same data and supporting code as the rule-based classifier.\n",
    "Please see that notebook for details on the data.\n",
    "\n",
    "Unlike the rule-based classifier, this BoW classifier will use automatically extracted features and _learn_ the weights on these features.\n",
    "Our features will be specifically be count vectors where each index refers to the number of a given word found in the input string.\n",
    "This is referred to of as a \"bag of words\" because it's like throwing all of the words into a bag and counting them -- while this method is simple, the main disadvantage is that we lose all structural information present in the sentence(s).\n",
    "We can then use our training data to learn weights between the individual words and our positive, negative, and neutral labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reading\n",
    "\n",
    "Read in the data from the training and dev (or finally test) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_XY_data(filename):\n",
    "    X_data = []\n",
    "    Y_data = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            label, text = line.strip().split(' ||| ')\n",
    "            X_data.append(text)\n",
    "            Y_data.append(int(label))\n",
    "    return X_data, Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = read_XY_data('../data/sst-sentiment-text-threeclass/train.txt')\n",
    "X_test, Y_test = read_XY_data('../data/sst-sentiment-text-threeclass/dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(datum):\n",
    "    # Split string into words\n",
    "    return datum.split(\" \")\n",
    "\n",
    "def build_feature_map(X):\n",
    "    # We need to assign an index to each word in order to build the count vector.\n",
    "    # We start by gathering a set of all word types in the training data.\n",
    "    word_types = set()\n",
    "    for datum in X:\n",
    "        for word in tokenize(datum):\n",
    "            word_types.add(word)\n",
    "    # Create a dictionary keyed by word mapping it to an index\n",
    "    return {word: idx for idx, word in enumerate(word_types)}\n",
    "            \n",
    "\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "def extract_features(word_to_idx, X):\n",
    "    # We are using a sparse matrix from scipy to avoid creating an 8000 x 18000 matrix\n",
    "    features = dok_matrix((len(X), len(word_to_idx)))\n",
    "    for i in range(len(X)):\n",
    "        for word in tokenize(X[i]):\n",
    "            if word in word_to_idx:\n",
    "                # Increment the word count if it is present in the map.\n",
    "                # Unknown words are discarded because we would not have\n",
    "                # a learned weight for them anyway.\n",
    "                features[i, word_to_idx[word]] += 1\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = [\n",
    "    \"When is the homework due ?\",\n",
    "    \"When are the TAs' office hours ?\",\n",
    "    \"How hard is the homework ?\",\n",
    "]\n",
    "\n",
    "word_to_idx = build_feature_map(sample_data)\n",
    "print(word_to_idx)\n",
    "print()\n",
    "\n",
    "features = extract_features(word_to_idx, sample_data)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the feature extractor on the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the map based on the training data\n",
    "word_to_idx = build_feature_map(X_train)\n",
    "\n",
    "print(f\"Unique word types in X_train: {len(word_to_idx)}\")\n",
    "print(\"Sample words:\")\n",
    "print(list(word_to_idx.keys())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our strings into count vectors\n",
    "X_train_vec = extract_features(word_to_idx, X_train)\n",
    "X_test_vec = extract_features(word_to_idx, X_test)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(tol=1e1)\n",
    "classifier.fit(X_train_vec, Y_train)\n",
    "\n",
    "# Create a truncated version of the training set so we have a second model to compare to\n",
    "X_train_vec_trunc = extract_features(word_to_idx, [x[:100] for x in  X_train])\n",
    "classifier_trunc = LogisticRegression(tol=1e1)\n",
    "classifier_trunc.fit(X_train_vec_trunc, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for statistical significance\n",
    "\n",
    "Now that we have trained our two models, it is tempting to just evaluate the accuarcy and see which one is better.\n",
    "The problem here is that we do not know if the difference in accuracy between the two models is statistically significant or not -- the difference might just be random chance.\n",
    "\n",
    "There are many different kinds of tests for statistical significance, but we will implement and perform just one here: pairwise bootstrapping.\n",
    "Bootstraping, generally speaking, refers to randomly sampling a subset of some data, calculating a staistic on the subset (e.g., taking their mean) and looking at the distribution of that statistic.\n",
    "With pairwise bootstrapping, we are going to randomly select a subset of test labels for the true labels and the two system we want to compare; we'll then see which system has the higher accuracy.\n",
    "We'll do this a number of times (i.e., 10,000) to get a better sense of the statistical significance of the difference between our systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "\n",
    "def eval_with_paired_bootstrap(gold, sys1, sys2, num_samples=10000, sample_ratio=0.5):\n",
    "    \"\"\"Evaluate with paired boostrap\n",
    "    This compares two systems, performing a significance tests with\n",
    "    paired bootstrap resampling to compare the accuracy of the two systems.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gold\n",
    "      The correct labels\n",
    "    sys1\n",
    "      The output of system 1\n",
    "    sys2\n",
    "      The output of system 2\n",
    "    num_samples\n",
    "      The number of bootstrap samples to take\n",
    "    sample_ratio\n",
    "      The ratio of samples to take every time\n",
    "\n",
    "    \"\"\"\n",
    "    assert len(gold) == len(sys1)\n",
    "    assert len(gold) == len(sys2)\n",
    "\n",
    "    gold = np.array(gold)\n",
    "    sys1 = np.array(sys1)\n",
    "    sys2 = np.array(sys2)\n",
    "\n",
    "    sys1_scores = []\n",
    "    sys2_scores = []\n",
    "    wins = [0, 0, 0]\n",
    "    n = len(gold)\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        # Subsample the gold and system outputs\n",
    "        subset_idxs = rng.choice(n, int(n * sample_ratio), replace=True)\n",
    "        sys1_score = (sys1[subset_idxs] == gold[subset_idxs]).mean()\n",
    "        sys2_score = (sys2[subset_idxs] == gold[subset_idxs]).mean()\n",
    "\n",
    "        if sys1_score > sys2_score:\n",
    "            wins[0] += 1\n",
    "        elif sys1_score < sys2_score:\n",
    "            wins[1] += 1\n",
    "        else:\n",
    "            wins[2] += 1\n",
    "\n",
    "        sys1_scores.append(sys1_score)\n",
    "        sys2_scores.append(sys2_score)\n",
    "\n",
    "    # Print win stats\n",
    "    wins = [x / float(num_samples) for x in wins]\n",
    "    print(\"Win ratio: sys1=%.3f, sys2=%.3f, tie=%.3f\" % (wins[0], wins[1], wins[2]))\n",
    "    if wins[0] > wins[1]:\n",
    "        print(\"(sys1 is superior with p value p=%.3f)\\n\" % (1 - wins[0]))\n",
    "    elif wins[1] > wins[0]:\n",
    "        print(\"(sys2 is superior with p value p=%.3f)\\n\" % (1 - wins[1]))\n",
    "\n",
    "    # Print system stats\n",
    "    sys1_scores.sort()\n",
    "    sys2_scores.sort()\n",
    "    print(\n",
    "        \"sys1 mean=%.3f, median=%.3f, 95%% confidence interval=[%.3f, %.3f]\"\n",
    "        % (\n",
    "            np.mean(sys1_scores),\n",
    "            np.median(sys1_scores),\n",
    "            sys1_scores[int(num_samples * 0.025)],\n",
    "            sys1_scores[int(num_samples * 0.975)],\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"sys2 mean=%.3f, median=%.3f, 95%% confidence interval=[%.3f, %.3f]\"\n",
    "        % (\n",
    "            np.mean(sys2_scores),\n",
    "            np.median(sys2_scores),\n",
    "            sys2_scores[int(num_samples * 0.025)],\n",
    "            sys2_scores[int(num_samples * 0.975)],\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win ratio: sys1=1.000, sys2=0.000, tie=0.000\n",
      "(sys1 is superior with p value p=0.000)\n",
      "\n",
      "sys1 mean=0.597, median=0.596, 95% confidence interval=[0.569, 0.625]\n",
      "sys2 mean=0.403, median=0.404, 95% confidence interval=[0.375, 0.433]\n",
      "\n",
      "Win ratio: sys1=0.777, sys2=0.169, tie=0.053\n",
      "(sys1 is superior with p value p=0.223)\n",
      "\n",
      "sys1 mean=0.596, median=0.596, 95% confidence interval=[0.567, 0.627]\n",
      "sys2 mean=0.588, median=0.589, 95% confidence interval=[0.558, 0.618]\n"
     ]
    }
   ],
   "source": [
    "cls_preds = classifier.predict(X_test_vec)\n",
    "cls_trunc_preds = classifier_trunc.predict(X_test_vec)\n",
    "baseline_preds = np.ones_like(cls_preds)\n",
    "\n",
    "eval_with_paired_bootstrap(Y_test, cls_preds, baseline_preds)\n",
    "print()\n",
    "eval_with_paired_bootstrap(Y_test, cls_preds, cls_trunc_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our full classifier is significantly better than the baseline, we cannot say that the full model is significantly better than the truncated model since the p-value is above 0.05 (the lower, the more significant)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (python3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
